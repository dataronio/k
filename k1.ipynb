{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgbm\n",
    "import xgboost as xgb\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Suppress warnings from pandas\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input files:\n",
      "['bureau.csv', 'application_test.csv', 'credit_card_balance.csv', 'previous_application.csv', 'installments_payments.csv', 'bureau_balance.csv', 'POS_CASH_balance.csv', 'application_train.csv']\n",
      "Loading data sets...\n"
     ]
    }
   ],
   "source": [
    "\"\"\" Load and process inputs \"\"\"\n",
    "input_dir = os.path.join(os.curdir, 'data')\n",
    "print('Input files:\\n{}'.format(os.listdir(input_dir)))\n",
    "print('Loading data sets...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_mem_usage(df):\n",
    "    \"\"\" iterate through all the columns of a dataframe and modify the data type\n",
    "        to reduce memory usage.        \n",
    "    \"\"\"\n",
    "    start_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n",
    "    \n",
    "    for col in df.columns:\n",
    "        col_type = df[col].dtype\n",
    "        \n",
    "        if col_type != object:\n",
    "            c_min = df[col].min()\n",
    "            c_max = df[col].max()\n",
    "            if str(col_type)[:3] == 'int':\n",
    "                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "                    df[col] = df[col].astype(np.int8)\n",
    "                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "                    df[col] = df[col].astype(np.int16)\n",
    "                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "                    df[col] = df[col].astype(np.int32)\n",
    "                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n",
    "                    df[col] = df[col].astype(np.int64)  \n",
    "            else:\n",
    "                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n",
    "                    df[col] = df[col].astype(np.float16)\n",
    "                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "                    df[col] = df[col].astype(np.float32)\n",
    "                else:\n",
    "                    df[col] = df[col].astype(np.float64)\n",
    "\n",
    "    end_mem = df.memory_usage().sum() / 1024**2\n",
    "    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n",
    "    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) / start_mem))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory usage of dataframe is 286.23 MB\n",
      "Memory usage after optimization is: 92.38 MB\n",
      "Decreased by 67.7%\n",
      "Memory usage of dataframe is 45.00 MB\n",
      "Memory usage after optimization is: 14.60 MB\n",
      "Decreased by 67.6%\n",
      "Memory usage of dataframe is 222.62 MB\n",
      "Memory usage after optimization is: 112.95 MB\n",
      "Decreased by 49.3%\n",
      "Memory usage of dataframe is 624.85 MB\n",
      "Memory usage after optimization is: 338.46 MB\n",
      "Decreased by 45.8%\n",
      "Memory usage of dataframe is 673.88 MB\n",
      "Memory usage after optimization is: 289.33 MB\n",
      "Decreased by 57.1%\n",
      "Memory usage of dataframe is 610.43 MB\n",
      "Memory usage after optimization is: 238.45 MB\n",
      "Decreased by 60.9%\n",
      "Memory usage of dataframe is 471.48 MB\n",
      "Memory usage after optimization is: 309.01 MB\n",
      "Decreased by 34.5%\n",
      "Memory usage of dataframe is 830.41 MB\n",
      "Memory usage after optimization is: 311.40 MB\n",
      "Decreased by 62.5%\n",
      "Data loaded.\n",
      "Main application training data set shape = (307511, 122)\n",
      "Main application test data set shape = (48744, 121)\n",
      "Positive target proportion = 0.08\n"
     ]
    }
   ],
   "source": [
    "sample_size = None\n",
    "app_train_df = reduce_mem_usage(pd.read_csv(os.path.join(input_dir, 'application_train.csv'), nrows=sample_size))\n",
    "app_test_df = reduce_mem_usage(pd.read_csv(os.path.join(input_dir, 'application_test.csv'), nrows=sample_size))\n",
    "bureau_df = reduce_mem_usage(pd.read_csv(os.path.join(input_dir, 'bureau.csv'), nrows=sample_size))\n",
    "bureau_balance_df = reduce_mem_usage(pd.read_csv(os.path.join(input_dir, 'bureau_balance.csv'), nrows=sample_size))\n",
    "credit_card_df = reduce_mem_usage(pd.read_csv(os.path.join(input_dir, 'credit_card_balance.csv'), nrows=sample_size))\n",
    "pos_cash_df = reduce_mem_usage(pd.read_csv(os.path.join(input_dir, 'POS_CASH_balance.csv'), nrows=sample_size))\n",
    "prev_app_df = reduce_mem_usage(pd.read_csv(os.path.join(input_dir, 'previous_application.csv'), nrows=sample_size))\n",
    "install_df = reduce_mem_usage(pd.read_csv(os.path.join(input_dir, 'installments_payments.csv'), nrows=sample_size))\n",
    "print('Data loaded.\\nMain application training data set shape = {}'.format(app_train_df.shape))\n",
    "print('Main application test data set shape = {}'.format(app_test_df.shape))\n",
    "print('Positive target proportion = {:.2f}'.format(app_train_df['TARGET'].mean()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_and_merge(left_df, right_df, agg_method, right_suffix):\n",
    "    \"\"\" Aggregate a df by 'SK_ID_CURR' and merge it onto another.\n",
    "    This method allows feature name \"\"\"\n",
    "    \n",
    "    agg_df = right_df.groupby('SK_ID_CURR').agg(agg_method)\n",
    "    merged_df = left_df.merge(agg_df, left_on='SK_ID_CURR', right_index=True, how='left',\n",
    "                              suffixes=['', '_' + right_suffix + agg_method.upper()])\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(app_data, bureau_df, bureau_balance_df, credit_card_df,\n",
    "                        pos_cash_df, prev_app_df, install_df):\n",
    "    \"\"\" \n",
    "    Process the input dataframes into a single one containing all the features. Requires\n",
    "    a lot of aggregating of the supplementary datasets such that they have an entry per\n",
    "    customer.\n",
    "    \n",
    "    Also, add any new features created from the existing ones\n",
    "    \"\"\"\n",
    "    \n",
    "    # # Add new features\n",
    "    \n",
    "    # Amount loaned relative to salary\n",
    "    app_data['LOAN_INCOME_RATIO'] = app_data['AMT_CREDIT'] / app_data['AMT_INCOME_TOTAL']\n",
    "    app_data['ANNUITY_INCOME_RATIO'] = app_data['AMT_ANNUITY'] / app_data['AMT_INCOME_TOTAL']\n",
    "    app_data['CONSUMER_GOODS_RATIO'] = app_data['AMT_CREDIT'] / app_data['AMT_GOODS_PRICE']\n",
    "    \n",
    "    # A lot of the continuous days variables have integers as missing value indicators.\n",
    "    # DWS add in missingness columns\n",
    "    app_data['DAYS_EMPLOYED_MISSING'] = np.where(app_data['DAYS_EMPLOYED']==365243, 1, 0)\n",
    "    app_data['DAYS_EMPLOYED'].replace(365243, 0, inplace= True)\n",
    "    prev_app_df['DAYS_LAST_DUE_MISSING'] = np.where(prev_app_df['DAYS_LAST_DUE']==365243, 1, 0)\n",
    "    prev_app_df['DAYS_LAST_DUE'].replace(365243, 0, inplace=True)\n",
    "    prev_app_df['DAYS_TERMINATION_MISSING'] = np.where(prev_app_df['DAYS_TERMINATION']==365243, 1, 0)\n",
    "    prev_app_df['DAYS_TERMINATION'].replace(365243, np.nan, inplace=True)\n",
    "    prev_app_df['DAYS_FIRST_DRAWING_MISSING'] = np.where(prev_app_df['DAYS_FIRST_DRAWING']==365243, 1, 0)\n",
    "    prev_app_df['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace=True)\n",
    "    prev_app_df['DAYS_FIRST_DUE_MISSING'] = np.where(prev_app_df['DAYS_FIRST_DUE']==365243, 1, 0)\n",
    "    prev_app_df['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace=True)\n",
    "    prev_app_df['DAYS_LAST_DUE_1ST_VERSION_MISSING'] = np.where(prev_app_df['DAYS_LAST_DUE_1ST_VERSION']==365243, 1, 0)\n",
    "    prev_app_df['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace=True)\n",
    "    \n",
    "    # Number of overall payments (I think!)\n",
    "    app_data['ANNUITY LENGTH'] = app_data['AMT_CREDIT'] / app_data['AMT_ANNUITY']\n",
    "    app_data['ANN_LENGTH_EMPLOYED_RATIO'] = app_data['ANNUITY LENGTH'] / app_data['DAYS_EMPLOYED']\n",
    "    \n",
    "    # Social features\n",
    "    app_data['WORKING_LIFE_RATIO'] = app_data['DAYS_EMPLOYED'] / app_data['DAYS_BIRTH']\n",
    "    app_data['INCOME_PER_FAM'] = app_data['AMT_INCOME_TOTAL'] / app_data['CNT_FAM_MEMBERS']\n",
    "    app_data['CHILDREN_RATIO'] = app_data['CNT_CHILDREN'] / app_data['CNT_FAM_MEMBERS']\n",
    "    \n",
    "    # Previous loan amounts & installments\n",
    "    prev_app_df['ASK_AMT_RATIO'] = prev_app_df['AMT_APPLICATION'] / prev_app_df['AMT_CREDIT']\n",
    "    prev_app_df['APPROVED'] = (prev_app_df['NAME_CONTRACT_STATUS'] == 'Approved').astype(int)\n",
    "    prev_app_df['REFUSED'] = (prev_app_df['NAME_CONTRACT_STATUS'] == 'Refused').astype(int)\n",
    "    install_df['DAYS_INSTALLMENT_LATE'] = install_df['DAYS_INSTALMENT'] - install_df['DAYS_ENTRY_PAYMENT']\n",
    "    install_df['PAYMENT_DISCREPANCY'] = install_df['AMT_INSTALMENT'] - install_df['AMT_PAYMENT']\n",
    "    \n",
    "    # Misc\n",
    "    app_data['TOTAL_DOCS_SUBMITTED'] = app_data.loc[:, app_data.columns.str.contains('FLAG_DOCUMENT')].sum(axis=1)\n",
    "    \n",
    "    # # Aggregate and merge supplementary datasets\n",
    "\n",
    "    # Previous applications\n",
    "    print('Combined train & test input shape before any merging  = {}'.format(app_data.shape))\n",
    "    agg_funs = {'SK_ID_CURR': 'count', 'AMT_CREDIT': 'sum'}\n",
    "    prev_apps = prev_app_df.groupby('SK_ID_CURR').agg(agg_funs)\n",
    "    prev_apps.columns = ['PREV APP COUNT', 'TOTAL PREV LOAN AMT']\n",
    "    merged_df = app_data.merge(prev_apps, left_on='SK_ID_CURR', right_index=True, how='left')\n",
    "\n",
    "    # Average the rest of the previous app data\n",
    "    for agg_method in ['mean', 'max', 'min']:\n",
    "        merged_df = agg_and_merge(merged_df, prev_app_df, agg_method, 'PRV')\n",
    "    print('Shape after merging with previous apps num data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Previous app categorical features\n",
    "    prev_app_df, cat_feats, _ = process_dataframe(prev_app_df)\n",
    "    prev_apps_cat_avg = prev_app_df[cat_feats + ['SK_ID_CURR']].groupby('SK_ID_CURR')\\\n",
    "                             .agg({k: lambda x: str(x.mode().iloc[0]) for k in cat_feats})\n",
    "    merged_df = merged_df.merge(prev_apps_cat_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_BAVG'])\n",
    "    print('Shape after merging with previous apps cat data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Credit card data - numerical features\n",
    "    wm = lambda x: np.average(x, weights=-1/credit_card_df.loc[x.index, 'MONTHS_BALANCE'])\n",
    "    credit_card_avgs = credit_card_df.groupby('SK_ID_CURR').agg(wm)   \n",
    "    merged_df = merged_df.merge(credit_card_avgs, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_CC_WAVG'])\n",
    "    for agg_method in ['mean', 'max', 'min']:\n",
    "        merged_df = agg_and_merge(merged_df, credit_card_avgs, agg_method, 'CC')\n",
    "    print('Shape after merging with previous apps num data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Credit card data - categorical features\n",
    "    most_recent_index = credit_card_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n",
    "    cat_feats = credit_card_df.columns[credit_card_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n",
    "    merged_df = merged_df.merge(credit_card_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n",
    "                                how='left', suffixes=['', '_CCAVG'])\n",
    "    print('Shape after merging with credit card data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Credit bureau data - numerical features\n",
    "    for agg_method in ['mean', 'max', 'min']:\n",
    "        merged_df = agg_and_merge(merged_df, bureau_df, agg_method, 'B')\n",
    "    print('Shape after merging with credit bureau data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Credit bureau categorical features\n",
    "    bureau_df, cat_feats, _ = process_dataframe(bureau_df)\n",
    "    bureau_cat_avg = bureau_df[cat_feats + ['SK_ID_CURR']].groupby('SK_ID_CURR')\\\n",
    "                               .agg({k: lambda x: str(x.mode().iloc[0]) for k in cat_feats})\n",
    "    merged_df = merged_df.merge(bureau_cat_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_BAVG'])\n",
    "    print('Shape after merging with credit bureau cat data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Bureau balance data\n",
    "    most_recent_index = bureau_balance_df.groupby('SK_ID_BUREAU')['MONTHS_BALANCE'].idxmax()\n",
    "    bureau_balance_df = bureau_balance_df.loc[most_recent_index, :]\n",
    "    merged_df = merged_df.merge(bureau_balance_df, left_on='SK_ID_BUREAU', right_on='SK_ID_BUREAU',\n",
    "                            how='left', suffixes=['', '_B_B'])\n",
    "    print('Shape after merging with bureau balance data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Pos cash data - weight values by recency when averaging\n",
    "    wm = lambda x: np.average(x, weights=-1/pos_cash_df.loc[x.index, 'MONTHS_BALANCE'])\n",
    "    f = {'CNT_INSTALMENT': wm, 'CNT_INSTALMENT_FUTURE': wm, 'SK_DPD': wm, 'SK_DPD_DEF':wm}\n",
    "    cash_avg = pos_cash_df.groupby('SK_ID_CURR')['CNT_INSTALMENT','CNT_INSTALMENT_FUTURE',\n",
    "                                                 'SK_DPD', 'SK_DPD_DEF'].agg(f)\n",
    "    merged_df = merged_df.merge(cash_avg, left_on='SK_ID_CURR', right_index=True,\n",
    "                                how='left', suffixes=['', '_CAVG'])\n",
    "                                \n",
    "    # Unweighted aggregations of numeric features\n",
    "    for agg_method in ['mean', 'max', 'min']:\n",
    "        merged_df = agg_and_merge(merged_df, pos_cash_df, agg_method, 'PC')\n",
    "    \n",
    "    # Pos cash data data - categorical features\n",
    "    most_recent_index = pos_cash_df.groupby('SK_ID_CURR')['MONTHS_BALANCE'].idxmax()\n",
    "    cat_feats = pos_cash_df.columns[pos_cash_df.dtypes == 'object'].tolist()  + ['SK_ID_CURR']\n",
    "    merged_df = merged_df.merge(pos_cash_df.loc[most_recent_index, cat_feats], left_on='SK_ID_CURR', right_on='SK_ID_CURR',\n",
    "                       how='left', suffixes=['', '_CAVG'])\n",
    "    print('Shape after merging with pos cash data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # Installments data\n",
    "    for agg_method in ['mean', 'max', 'min']:\n",
    "        merged_df = agg_and_merge(merged_df, install_df, agg_method, 'I')    \n",
    "    print('Shape after merging with installments data = {}'.format(merged_df.shape))\n",
    "    \n",
    "    # Add more value counts\n",
    "    merged_df = merged_df.merge(pd.DataFrame(bureau_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_BUREAU'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(credit_card_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_CRED_CARD'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(pos_cash_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_POS_CASH'])\n",
    "    merged_df = merged_df.merge(pd.DataFrame(install_df['SK_ID_CURR'].value_counts()), left_on='SK_ID_CURR', \n",
    "                                right_index=True, how='left', suffixes=['', '_CNT_INSTALL'])\n",
    "    print('Shape after merging with counts data = {}'.format(merged_df.shape))\n",
    "\n",
    "    # # Further engineering of combined features\n",
    "    merged_df['BALANCE_TO_ANNUITY_RATIO'] = merged_df['AMT_ANNUITY'] / merged_df['AMT_BALANCE']\n",
    "    merged_df['RATIO_TO_PREV_AMTS'] = merged_df['AMT_CREDIT'] / merged_df['AMT_CREDIT_PRVMEAN']\n",
    "\n",
    "    return merged_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_dataframe(input_df, encoder_dict=None):\n",
    "    \"\"\" Process a dataframe into a form useable by LightGBM \"\"\"\n",
    "\n",
    "    # Label encode categoricals\n",
    "    print('Label encoding categorical features...')\n",
    "    categorical_feats = input_df.columns[input_df.dtypes == 'object']\n",
    "    for feat in categorical_feats:\n",
    "        encoder = LabelEncoder()\n",
    "        input_df[feat] = encoder.fit_transform(input_df[feat].fillna('NULL'))\n",
    "    print('Label encoding complete.')\n",
    "\n",
    "    return input_df, categorical_feats.tolist(), encoder_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined train & test input shape before any merging  = (356255, 132)\n",
      "Shape after merging with previous apps num data = (356255, 246)\n",
      "Label encoding categorical features...\n",
      "Label encoding complete.\n",
      "Shape after merging with previous apps cat data = (356255, 262)\n",
      "Shape after merging with previous apps num data = (356255, 346)\n",
      "Shape after merging with credit card data = (356255, 347)\n",
      "Shape after merging with credit bureau data = (356255, 392)\n",
      "Label encoding categorical features...\n",
      "Label encoding complete.\n",
      "Shape after merging with credit bureau cat data = (356255, 395)\n",
      "Shape after merging with bureau balance data = (356255, 397)\n",
      "Shape after merging with pos cash data = (356255, 422)\n",
      "Shape after merging with installments data = (356255, 449)\n",
      "Shape after merging with counts data = (356255, 453)\n"
     ]
    }
   ],
   "source": [
    "# Merge the datasets into a single one for training\n",
    "len_train = len(app_train_df)\n",
    "app_both = pd.concat([app_train_df, app_test_df])\n",
    "merged_df = feature_engineering(app_both, bureau_df, bureau_balance_df, credit_card_df,\n",
    "                                pos_cash_df, prev_app_df, install_df)\n",
    "merged_df.to_csv('processed_input_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label encoding categorical features...\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-acf433f3b367>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# Process the data set.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmerged_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcategorical_feats\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoder_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmerged_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Capture other categorical features not as object data types:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f43965e2e56a>\u001b[0m in \u001b[0;36mprocess_dataframe\u001b[0;34m(input_df, encoder_dict)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_feats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLabelEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0minput_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NULL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Label encoding complete.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/thinkful/lib/python3.6/site-packages/sklearn/preprocessing/label.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, y)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \"\"\"\n\u001b[1;32m    111\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/thinkful/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Invalid axis kwarg specified for unique'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/thinkful/lib/python3.6/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    272\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0moptional_indices\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m         \u001b[0mperm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mergesort'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_index\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'quicksort'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'float'"
     ]
    }
   ],
   "source": [
    "# Separate metadata\n",
    "meta_cols = ['SK_ID_CURR']\n",
    "meta_df = merged_df[meta_cols]\n",
    "merged_df.drop(columns=meta_cols, inplace=True)\n",
    "\n",
    "# Process the data set.\n",
    "merged_df, categorical_feats, encoder_dict = process_dataframe(input_df=merged_df)\n",
    "\n",
    "# Capture other categorical features not as object data types:\n",
    "non_obj_categoricals = [\n",
    "    'FONDKAPREMONT_MODE', 'HOUR_APPR_PROCESS_START', 'HOUSETYPE_MODE',\n",
    "    'NAME_EDUCATION_TYPE', 'NAME_FAMILY_STATUS', 'NAME_HOUSING_TYPE',\n",
    "    'NAME_INCOME_TYPE', 'NAME_TYPE_SUITE', 'OCCUPATION_TYPE',\n",
    "    'ORGANIZATION_TYPE', 'STATUS', 'NAME_CONTRACT_STATUS_CAVG',\n",
    "    'WALLSMATERIAL_MODE', 'WEEKDAY_APPR_PROCESS_START', 'NAME_CONTRACT_TYPE_BAVG',\n",
    "    'WEEKDAY_APPR_PROCESS_START_BAVG', 'NAME_CASH_LOAN_PURPOSE', 'NAME_CONTRACT_STATUS', \n",
    "    'NAME_PAYMENT_TYPE', 'CODE_REJECT_REASON', 'NAME_TYPE_SUITE_BAVG', \n",
    "    'NAME_CLIENT_TYPE', 'NAME_GOODS_CATEGORY', 'NAME_PORTFOLIO', \n",
    "    'NAME_PRODUCT_TYPE', 'CHANNEL_TYPE', 'NAME_SELLER_INDUSTRY', \n",
    "    'NAME_YIELD_GROUP', 'PRODUCT_COMBINATION', 'NAME_CONTRACT_STATUS_CCAVG',\n",
    "    'CREDIT_ACTIVE', 'CREDIT_CURRENCY', 'CREDIT_TYPE'\n",
    "]\n",
    "categorical_feats = categorical_feats + non_obj_categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-separate into train and test\n",
    "train_df = merged_df[:len_train]\n",
    "test_df = merged_df[len_train:]\n",
    "del merged_df, app_test_df, app_train_df, bureau_df, bureau_balance_df, credit_card_df, pos_cash_df, prev_app_df\n",
    "gc.collect()\n",
    "\n",
    "\"\"\" Train the LightGBM model \"\"\"\n",
    "target = train_df.pop('TARGET')\n",
    "test_df.drop(columns='TARGET', inplace=True)\n",
    "lgbm_train = lgbm.Dataset(data=train_df,\n",
    "                          label=target,\n",
    "                          categorical_feature=categorical_feats,\n",
    "                          free_raw_data=False)\n",
    "                          \n",
    "lgbm_params = {\n",
    "    'boosting': 'dart',\n",
    "    'application': 'binary',\n",
    "    'learning_rate': 0.1,\n",
    "    'min_data_in_leaf': 30,\n",
    "    'num_leaves': 31,\n",
    "    'max_depth': -1,\n",
    "    'feature_fraction': 0.5,\n",
    "    'scale_pos_weight': 2,\n",
    "    'drop_rate': 0.02\n",
    "}\n",
    "\n",
    "cv_results = lgbm.cv(train_set=lgbm_train,\n",
    "                     params=lgbm_params,\n",
    "                     nfold=5,\n",
    "                     num_boost_round=600,\n",
    "                     early_stopping_rounds=50,\n",
    "                     stratified=True,\n",
    "                     verbose_eval=50,\n",
    "                     metrics=['auc'])\n",
    "\n",
    "optimum_boost_rounds = np.argmax(cv_results['auc-mean'])\n",
    "print('Optimum boost rounds = {}'.format(optimum_boost_rounds))\n",
    "print('Best LGBM CV result = {}'.format(np.max(cv_results['auc-mean'])))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
